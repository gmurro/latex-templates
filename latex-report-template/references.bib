@article{knuth74,
    author = {Knuth, Donald E.},
    title = {Computer Programming As an Art},
    journal = {Commun. ACM},
    year = {1974},
    pages = {667--673},
    numpages = {7},
    publisher = {ACM},
    address = {New York, NY, USA}
}

@article{knuth92,
    author = {Knuth, Donald E.},
    title = {Two notes on notation},
    journal = {Amer. Math. Monthly},
    volume = {99},
    year = {1992},
    pages = {403--422}
}

@book{lamport94,
    title = {LaTeX: A Document Preparation System},
    author = {Lamport, Leslie},
    year = {1994},
    publisher = {Pearson Education India}
}

@misc{bert,
  abstract  = {Abstract of Paper},
  added-at  = {2019-02-05T23:35:51.000+0100},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords  = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  note      = {cite arxiv:1810.04805Comment: 13 pages},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {http://arxiv.org/abs/1810.04805},
  year      = 2018
}



@online{bidaf-medium,
  author = {Meraldo Antonio},
  title  = {An Illustrated Guide to Bi-Directional Attention Flow (BiDAF)},
  url    = {https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b}
}




@online{colab,
  author = {Google},
  title  = {Colaboratory},
  url    = {https://colab.research.google.com/}
}



@article{huggingface,
  author        = {Thomas Wolf and
               Lysandre Debut and
               Victor Sanh and
               Julien Chaumond and
               Clement Delangue and
               Anthony Moi and
               Pierric Cistac and
               Tim Rault and
               R{\'{e}}mi Louf and
               Morgan Funtowicz and
               Jamie Brew},
  title         = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal       = {CoRR},
  volume        = {abs/1910.03771},
  year          = {2019},
  url           = {http://arxiv.org/abs/1910.03771},
  archiveprefix = {arXiv},
  eprint        = {1910.03771},
  timestamp     = {Tue, 02 Jun 2020 12:49:01 +0200}
}


@article{lstm,
  author   = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal  = {Neural Computation},
  title    = {Long Short-Term Memory},
  year     = {1997},
  number   = {8},
  pages    = {1735--1780},
  volume   = {9},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  doi      = {10.1162/neco.1997.9.8.1735}
}

@techreport{max-context-tokens,
  title       = {{Question Answering on the SQuAD Dataset}},
  author      = {Do-Hyoung Park and Vihan Lakshman},
  institution = {Stanford University},
  month       = {01}
}

@inproceedings{ppmi,
  author    = {Niwa, Yoshiki and Nitta, Yoshihiko},
  title     = {Co-Occurrence Vectors from Corpora vs. Distance Vectors from Dictionaries},
  year      = {1994},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  url       = {https://doi.org/10.3115/991886.991938},
  doi       = {10.3115/991886.991938},
  abstract  = {A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions. The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal (20M total words) was higher than that by using distance vectors from the Collins English Dictionary (60K head words + 1.6M definition words). However, other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors.},
  booktitle = {Proceedings of the 15th Conference on Computational Linguistics - Volume 1},
  pages     = {304–309},
  numpages  = {6},
  series    = {COLING '94}
}

@incollection{pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}


@inbook{tf-idf,
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  title     = {TF--IDF},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {986--987},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_832},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_832}
}

